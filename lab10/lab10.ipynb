{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "* Reviews: Befana\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice, uniform\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loggin configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='tic_tac_toe_training_debug.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20_000\n",
    "test_game = 5_000\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6,\n",
    "         9, 5, 1,\n",
    "         4, 3, 8]\n",
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TicTacToeEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnvironment:\n",
    "    def reset(self):\n",
    "        self.board = State(set(), set())\n",
    "        self.current_player = \"X\"\n",
    "        self.available = [2, 7, 6,\n",
    "                          9, 5, 1,\n",
    "                          4, 3, 8]\n",
    "        return (self.board, self.current_player)\n",
    "    \n",
    "\n",
    "    def win(self, player):\n",
    "        if player == \"X\":\n",
    "            return any(sum(c) == 15 for c in combinations(self.board.x, 3)) # X wins if this expression returns true \n",
    "        \n",
    "        return any(sum(c) == 15 for c in combinations(self.board.o, 3))\n",
    "    \n",
    "    \n",
    "    def get_reward(self, done):\n",
    "        if done:\n",
    "            return 1\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()   \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                i = r * 3 + c\n",
    "                if MAGIC[i] in self.board.x:\n",
    "                    print('X', end='')\n",
    "                elif MAGIC[i] in self.board.o:\n",
    "                    print('O', end='')\n",
    "                else:\n",
    "                    print('.', end='')\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "       \n",
    "      \n",
    "        if self.current_player == \"X\":\n",
    "            self.board.x.add(action)\n",
    "            self.available.remove(action)  # Updated available moves\n",
    "        else:\n",
    "            self.board.o.add(action)\n",
    "            self.available.remove(action)\n",
    "\n",
    "        done = self.win(self.current_player)  # It keeps track if our episode if finished\n",
    "        reward = self.get_reward(done) # Compute the reward\n",
    "\n",
    "        if done:\n",
    "            info = {\"The player: \" + self.current_player + \" wins!\"}\n",
    "        else:\n",
    "            info = {\"The player: \" + self.current_player + \" ,does action: \" + str(action) }\n",
    "            self.current_player = \"O\" if self.current_player == \"X\" else \"X\"\n",
    "\n",
    "        return self.board, reward, done, info\n",
    "    \n",
    "    def compute(self, comb):\n",
    "        done = any(sum(c) == 15 for c in combinations(comb, 3)) \n",
    "        reward = self.get_reward(done)\n",
    "        return reward\n",
    "    \n",
    "    def evaluate_possible_outcome(self, new_state):\n",
    "        combs = [new_state.o.union({available_action}) for available_action in self.available]\n",
    "        possible_outcomes = map(self.compute, combs)\n",
    "        possible_outcomes_list = list(possible_outcomes)\n",
    "        print(f'possible_outcomes:  {combs}')\n",
    "        print(f'possible_outcomes value:  {possible_outcomes_list}')\n",
    "        return possible_outcomes_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table= defaultdict(lambda: [0.0] *9)\n",
    "#q_table = defaultdict(lambda: [random.gauss(0, 0.01) for _ in range(9)])   POSSIBLE INITIALIZATION WITH VALUES TAKEN FROM A GAUSSIAN DISTRIBUTION  WITH STANDARD DEVIATION = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning algorithm vs random player\n",
    "* X is the smart agent\n",
    "* O is the random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnvironment()\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state, player= env.reset() \n",
    "    current_state = deepcopy(state)\n",
    "    \n",
    "    rewards_current_episode = 0\n",
    "   \n",
    "\n",
    "    while True:\n",
    "        invalid = True\n",
    "       \n",
    "        logging.debug(f\"\\n*********** Player: {env.current_player} turn!********\\n\")\n",
    "        logging.debug(f'\\navailable: {env.available}')\n",
    "\n",
    "        if env.current_player == \"X\":\n",
    "\n",
    "            #Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = uniform(0,1)\n",
    "\n",
    "            if exploration_rate_threshold > exploration_rate:\n",
    "                \n",
    "                #The algorithnm can choose only available actions\n",
    "                # An action is available if it allows to write in a empty cell. \n",
    "                available_actions = [a - 1 for a in list(env.available)]  # My actions are in a range between 1 and 9 while in the q-table they are expressed in a list of 9 element which indexes start from 0 (to 8)\n",
    "                state_str = str(current_state) \n",
    "                available_q_values = [q_table[state_str][a] for a in available_actions]\n",
    "                action = available_actions[np.argmax(available_q_values)] + 1\n",
    "        \n",
    "            else:\n",
    "                action = choice(list(env.available)) \n",
    "                \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            new_state_str = str(new_state)\n",
    "            state_str = str(current_state)\n",
    "            logging.debug(f'current:  {current_state}')\n",
    "            logging.debug(info)\n",
    "            logging.debug(f'reward:  {reward}')\n",
    "            logging.debug(f'new:  {new_state}')\n",
    "        \n",
    "            \n",
    "            outcomes_from_future = env.evaluate_possible_outcome(new_state)\n",
    "            Q_S_t_next = 0 if not outcomes_from_future else outcomes_from_future\n",
    "            action -= 1\n",
    "            q_table[state_str][action] = q_table[state_str][action] * (1- learning_rate) + learning_rate * (reward + discount_rate * (-1.0 *np.max(Q_S_t_next)))\n",
    "       \n",
    "            rewards_current_episode += reward\n",
    "\n",
    "        else:\n",
    "            action = choice(list(env.available)) \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            logging.debug(f'current:  {current_state}')\n",
    "            logging.debug(info)\n",
    "            logging.debug(f'new:  {new_state}')\n",
    "\n",
    "        current_state = deepcopy(new_state)\n",
    "        \n",
    "        if done == True or not env.available: \n",
    "            break \n",
    "              \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in q_table.items():\n",
    "    print(f\"{key.ljust(40)} \\t{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and print the average reward per thousand episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = []\n",
    "y_axis = []\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)  # split into groups of 1000 episodes\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes*******\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    x_axis.append(count)\n",
    "    avg = sum(r/1000)\n",
    "    print(count, \" --> \", str(avg))\n",
    "    y_axis.append(avg)\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg rewad graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(x_axis, y_axis)\n",
    "\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('Average reward per thousand episodes')\n",
    "\n",
    "\n",
    "plt.title('Average reward over groups of 1000 episodes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_x = 0\n",
    "count_o = 0\n",
    "count_draw = 0\n",
    "\n",
    "for episode in range(test_game):\n",
    "    state, player = env.reset()\n",
    "    current_state = deepcopy(state)\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        state_str = str(current_state)\n",
    "\n",
    "        if env.current_player == \"X\":\n",
    "            available_actions = [a - 1 for a in list(env.available)]  \n",
    "            state_str = str(current_state) \n",
    "            available_q_values = [q_table[state_str][a] for a in available_actions]\n",
    "            action = available_actions[np.argmax(available_q_values)] + 1\n",
    "        else:\n",
    "            action = choice(list(env.available)) \n",
    "                 \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done or not env.available:\n",
    "            if reward == 1 and env.current_player == \"X\":\n",
    "                count_x += 1\n",
    "            elif reward == 1 and env.current_player == \"O\":\n",
    "                count_o += 1\n",
    "            else:\n",
    "                count_draw += 1\n",
    "            break\n",
    "\n",
    "        current_state = deepcopy(new_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Q-learning agent wins {count_x/test_game *100}%\\nRandom player wins: {count_o/test_game *100}%\\nDrawn: {count_draw/test_game*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(3):\n",
    "    state, player = env.reset()\n",
    "    current_state = deepcopy(state)\n",
    "    \n",
    "    print(\"*****Episode \", episode+1, \"******\\n\\n\")\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    while True:\n",
    "        clear_output(wait=True) \n",
    "      \n",
    "        print(f\"\\n*********** Player: {env.current_player} turn!********\\n\")\n",
    "        print('available: ', env.available)\n",
    "\n",
    "        state_str = str(current_state) \n",
    "        if env.current_player == \"X\":\n",
    "            available_actions = [a - 1 for a in list(env.available)]  \n",
    "            state_str = str(current_state) \n",
    "            available_q_values = [q_table[state_str][a] for a in available_actions]\n",
    "            action = available_actions[np.argmax(available_q_values)] + 1\n",
    "        else:\n",
    "            action = choice(list(env.available)) \n",
    "           \n",
    "    \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        print(info)\n",
    "        print(current_state)\n",
    "        print(new_state)\n",
    "        env.render()\n",
    "        time.sleep(3)\n",
    "\n",
    "        if done or not env.available:\n",
    "          \n",
    "            if reward == 1 and env.current_player == \"X\":\n",
    "                print(\"****X wins the game ****\")\n",
    "                time.sleep(2)\n",
    "            elif reward == 1 and env.current_player == \"O\":\n",
    "                print(\"****O wins the game****\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"****Draw****\")\n",
    "                time.sleep(2)\n",
    "            break\n",
    "\n",
    "        current_state = deepcopy(new_state)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
